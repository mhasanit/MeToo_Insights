{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Rightsatbirth: \"What he did was manipulative...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy 94th Birthday\\0\\x9f\\x8e\\x82 to @_AngelaL...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  word_count\n",
       "0   @Rightsatbirth: \"What he did was manipulative...          21\n",
       "1  Happy 94th Birthday\\0\\x9f\\x8e\\x82 to @_AngelaL...          18"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords') remove Comment tag and install this package\n",
    "#nltk.download('wordnet') remove Comment tag and install this package\n",
    "import pandas\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sb\n",
    "from os import path\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Load tweets to dataset\n",
    "df = pandas.read_csv('Metoo_tweets.csv')\n",
    "\n",
    "# some exploration to our tweets\n",
    "#Fetch how many word in each Tweet\n",
    "df['word_count'] = df['Text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df[['Text','word_count']].head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary exploratiog text \n",
    "#Fetch wordcount for each Tweet\n",
    "df['word_count'] = df['Text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df[['Text','word_count']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.word_count.describe()\n",
    "\n",
    "#df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many words in the dataset\n",
    "df['word_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get common words\n",
    "#Series.value_counts() return a Series containing counts of unique values in descending order, Excludes NA values by default.\n",
    "mfreq = pandas.Series(' '.join(df['Text']).split()).value_counts()[:25]\n",
    "mfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get uncommon words\n",
    "unfreq =  pandas.Series(' '.join(df \n",
    "         ['Text']).split()).value_counts()[-25:]\n",
    "unfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import stop words from package and adding custom stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#Creating a list of custom stopwords\n",
    "new_words = [\"http\",\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"nshe\",\"new\", \"previously\", \"shown\",'http',\"xa\",\"xe\",\"rt\",\"oct\",\"th\",\"co\",\"metoo\",\"amp\",\"ever\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise removal, Normalization(just removes the last few characters)\n",
    "#Lemmatisation(is the process of converting a word to its base form)\n",
    "\n",
    "#put each Tweet in  an array[i] in order to clean it(Noise Removal), \n",
    "corp = []\n",
    "for i in range(0, df.shape[0]):#df.shape[0] get how many rows in dataset\n",
    "    # 1Remove punctuations\n",
    "  \n",
    "    txt=re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",df['Text'][i])\n",
    "    p=re.compile(r'\\<http.+?\\>', re.DOTALL)\n",
    "    txt = re.sub(p, '', txt)\n",
    "    \n",
    "    # 2 -convert to lowercase\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # 3 -remove tags\n",
    "    txt=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",txt)\n",
    "    \n",
    "    # 4 -remove digits and\n",
    "    txt=re.sub(\"(\\\\d|\\\\W)+\",\" \",txt)\n",
    "    \n",
    "    # 5 -Create a list from string\n",
    "    txt = txt.split()\n",
    "    \n",
    "    # 6 -Doing Lemmatisation\n",
    "    #Lemmatisation it's better than Stemming\n",
    "    #Caring’ -> Lemmatization -> ‘Care’\n",
    "    #‘Caring’ -> Stemming -> ‘Car’\n",
    "    lemm = WordNetLemmatizer()\n",
    "    txt = [lemm.lemmatize(word) for word in txt if not word in  \n",
    "            stop_words] \n",
    "    txt = \" \".join(txt)\n",
    "    corp.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=1600, height=800).generate(str(corp))\n",
    "#  plot word cloud image.\n",
    "\n",
    "plt.figure( figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a vector of word counts\n",
    "#ignore terms that appear in more than 70% of the tweets\n",
    "cvec=CountVectorizer(max_df=0.7,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "Fit=cvec.fit_transform(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent single words after deleting stop words\n",
    "def fetch_top_nwords(corp, n=None):\n",
    "    vec = CountVectorizer().fit(corp)\n",
    "    words_bag = vec.transform(corp)\n",
    "    sum_words = words_bag.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "#Convert  freq words to dataset to plot bar plot\n",
    "top_words = fetch_top_nwords(corp, n=25)\n",
    "top_df = pandas.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Frequency\"]\n",
    "#plot  most freq words\n",
    "sb.set(rc={'figure.figsize':(13,8)})\n",
    "k = sb.barplot(x=\"Word\", y=\"Frequency\", data=top_df)\n",
    "k.set_xticklabels(k.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent  Bi-grams\n",
    "def fetch_top_n2words(corp, n=None):\n",
    "    vecn2 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corp)\n",
    "    words_bag = vecn2.transform(corp)\n",
    "    sum_words = words_bag.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vecn2.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top_2words = fetch_top_n2words(corp, n=20)\n",
    "top2_df = pandas.DataFrame(top_2words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Frequency\"]\n",
    "print(top2_df)\n",
    "#Plot most freq Bi-grams\n",
    "\n",
    "sb.set(rc={'figure.figsize':(13,8)})\n",
    "b=sb.barplot(x=\"Bi-gram\", y=\"Frequency\", data=top2_df)\n",
    "b.set_xticklabels(b.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getm ost frequent Tri-grams\n",
    "def fetch_top_n3words(corp, n=None):\n",
    "    vecn3 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corp)\n",
    "    words_bag = vecn3.transform(corp)\n",
    "    sum_words = words_bag.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vecn3.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top_3words = fetch_top_n3words(corp, n=20)\n",
    "top3_df = pandas.DataFrame(top_3words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Frequency\"]\n",
    "print(top3_df)\n",
    "#Plot  most freq Tri-grams\n",
    "\n",
    "sb.set(rc={'figure.figsize':(13,8)})\n",
    "p=sb.barplot(x=\"Tri-gram\", y=\"Frequency\", data=top3_df)\n",
    "p.set_xticklabels(p.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the most popular tweet\n",
    "df['Total']=df['Favorite_count']+df['Retweet_count']\n",
    "Mot_Popular=df[df['Total']>=df['Total'].max()]\n",
    "Mot_Popular[['Text','Total']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show number of tweets by device/mobile or platform comming from\n",
    "Source_Tweet = df.groupby(\"Source\")\n",
    "#Source_Tweet.count().sort_values(by=\"Source\",ascending=False)\n",
    "plt.figure(figsize=(15,10))\n",
    "Source_Tweet.size().sort_values(ascending=False).head(10).plot.bar()\n",
    "plt.xticks(rotation=50)\n",
    "plt.xlabel(\"Number of Tweets by its Source\")#.encode('utf-8')\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
